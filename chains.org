#+TITLE: Experiment Chains
#+AUTHOR: Mark Cox

~CHAINS~ is a common lisp system for making it easy to construct an
experiment chain in which a number of links are assembled to produce a
result. Its main purpose is to accommodate scenarios where the line
between a scientific experiment and algorithmic experimentation is
blurred.

[TABLE-OF-CONTENTS]

* A backstory
This section outlines the main use case for the ~CHAINS~ system. The
abstraction the ~CHAINS~ system employs is rather strange when
introduced on its own. Hopefully this section aids in understanding
why. The example presented in this section is real, and is the primary
reason for the existence of the ~CHAINS~ system.

An important problem in the field of computer vision is that of
/congealing/. The task of congealing is very simple, discover the
location of an object contained in an ensemble of images. A key trait
of congealing algorithms is that they know nothing about the object in
each image, other than it can be assumed that the object is present in
each image. A concrete example of congealing would be, given an
ensemble of images containing faces, find me the bounding box of each
face.

When evaluating a congealing algorithm one must evaluate the algorithm
against the following criteria
- Level of initial misalignment :: How far away from the initial guess
     can the algorithm recover the true location of the object?
- Lighting variation :: How does the algorithm handle variation in
     appearance due to illumination conditions?
- Image noise :: How does the algorithm handle variation in appearance
                 due to image noise?
- Appearance variation :: How does the algorithm handle variation in
     appearance of the object? For faces, this would mean male versus
     female, beard versus non-beard, and so on.
- Occlusion :: How does the algorithm handle cases where the object is
               occluded by another object? e.g. a face occluded by
               hair and/or glasses.
- Ensemble size :: How does the number of images in the ensemble
                   influence performance?
- Cross validation on images :: Testing performance on another ensemble of
     images with exactly the same statistics.
- Cross validation on geometry :: Testing performance on the same
     collection of images, but with different initial guesses of the
     true location of the object.

As you have probably discerned by know, the variables mentioned above
are all associated with statistics on the ensemble of images fed to
the congealing algorithm. The reader can assume that these statistics
are in fact controllable. i.e. it is possible to synthesise an
ensemble of images that satisfy the needed statistic.

In addition to variables associated with the input data, a congealing
algorithm typically has a number of parameters that influence its own
performance. These parameters have a range of values that are "worth"
trying.

A scientist also wants to compare the algorithm against another
benchmark algorithm on /exactly/ the same data. If no benchmark
algorithm exists, then a number of other algorithms are
evaluated. Having exactly the same data is extremely important. A
strategically chosen change in data can have a dramatic effect on
results for some algorithms.

Congealing algorithms are typically iterative algorithms. i.e. The
algorithm repeatedly processes its own output until some termination
criteria is satisfied.

Once all of the algorithms are executed, a results collation process
begins. What results that are obtained depend on the paper. These
results may involve performance conditioned on the statistics of the
image ensemble, or results indicating the influence each algorithm
parameter has on its performance.

Everything up to this point is straight forward. You prepare the data
to be fed to the algorithms, you execute the algorithms and then you
produce results (and then write the report/paper/journal). Easy.

Unfortunately, this rarely occurs in practice. 

In practice, one implements an algorithm that has been published. Upon
experimenting (or implementing) with the algorithm, it is discovered
that there is something that could be improved. For example, the
algorithm cannot handle image ensembles with certain statistics or the
algorithm is really sensitive to a particular parameter or the
algorithm is theoretically flawed in some manner. With this new
information, the goal is now to design a new algorithm, that can
overcome the identified algorithm.

Immediately we see that the algorithm and the needed experiments are
not concrete. One typically creates many algorithms that may "do the
job", but one does not know whether it is an idea worth investigating
further until it has been implemented and a preliminary set of results
are obtained. Additionally, the type of results required may change
depending on the point being made.

What a scientist wants to work on is the algorithm. That is what is
important. That is what is publishable. The experiment stuff validates
the argument for the algorithm, but it is not the primary focus. 

Saying it is not of primary focus is rather ironic. The experiment
design is what lead to the discovery. The experiment design also
validates a new discovery. Furthermore, it is the experiment design
where much of a computer scientist's effort is spent and it is to easy
to see why.

The problem with experiments is that they take time to
execute. Obviously this time is a function of the algorithm execution
time. It is also a function of the number of experiments, or what is
termed, the size of the experiment design. An experiment design
defines the experiments to be executed i.e. the different types of
input data, the different types of algorithms, and also the different
types of information required to compute results.

Tragically, the size of the experiment design is a combinatorial
function built from the number of values specified for each experiment
parameter. i.e. If you have a large number of experiment parameters
and a large number of values per experiment parameter then you have an
extremely large number of experiments.

Fortunately, since the experiments are independent, they can be
executed in parallel. Well not quite in parallel, but there are times
where parallelism is possible. Consider the following where a number
of congealing algorithms need to process the given input data.
#+begin_src ditaa :file parallelism.png
+--------------+     +-------------+
| Input Data 1 +-+-->| Algorithm 1 |
+--------------+ |   +-------------+
                 |
                 |   +-------------+
                 +-->| Algorithm 2 |
                     +-------------+

+--------------+     +-------------+
| Input Data 2 +-+-->| Algorithm 1 |
+--------------+ |   +-------------+
                 |
                 |   +-------------+
                 +-->| Algorithm 2 |
                     +-------------+
#+end_src
Once input data 1 and 2 are created, all four algorithm invocations
can be executed in parallel provided the algorithm implementation
permits.

For congealing, creating the input data is arguably the most
complicated component of the experimental setup as there are a number
of parameters that govern how the input data is formed. A
visualisation of this process is as follows
#+begin_src ditaa :file input-data.png
  +-------------------------------+
  | Image Appearance and Lighting |
  +------+------------------------+
         | 
         v
  +------------------------+
  | Image Cross Validation |<--=-Images selected here
  +------+-----------------+
         |
         v
  +---------------------+
  | Synthetic Occlusion |<--=-- Occluded regions added.
  +------+--------------+
         | 
         o<--=--Input data images created
         |
         v         
  +---------------------------------+
  | Distance away from ground truth |
  +------+--------------------------+
         |
         v
  +----------------------------+
  | Geometric Cross Validation |<--=-Create initial guess
  +----------------------------+
#+end_src
The term cross validation used above refers to the process of
synthesising a set of random samples with the same statistics. Thus
for the image cross validation, an entirely different set of images
with same level of appearance variation and lighting conditions are
selected. Similarly, the geometric cross validation is a new set
initial guesses on where the object is for the same set of
images. There should be a cross validation block for the synthetic
occlusion component too, but we have chosen to omit it for clarity.

For an experiment design, there will be many different image
appearance and lighting statistics used as well as many different
synthetic occlusions so the above /chain/ that creates the input data
is really a single path in a tree that defines the experiment design.

To complicate matters, there may be many other ways of selecting the
input data images. i.e. with no occlusion, with no cross validation,
with image noise and so on. Thus the marker "Input data images
created" really represents the input to the next stage of the
experiment design. In this case, the initial guess of where the object
is the image. Obviously, in order to form an initial guess, you need
to know the ground truth location of the object in the image. Assuming
this information is available, then synthesising an initial guess is
trivial.

Once the initial guess is created, then the algorithm can be
executed. The algorithm also needs the input data images, thus, the
input to the algorithm is really the input data images and the initial
guess as depicted in the next figure.
#+begin_src ditaa :file algorithm-input.png
         |
         o<--=--Input data images created
         |
+--------+
|        |
|        v                                                  
| +---------------------------------+
| | Distance away from ground truth |
| +------+--------------------------+
|        | 
|        v
| +----------------------------+
| | Geometric Cross Validation |
| +------+---------------------+
|        |
+---+    o<--=--Initial Guess
    |    |
    v    v
  +-----------+
  | Algorithm |
  +-----------+
#+end_src

What is obvious is that there are various stages along the chain that
define the transition from one component of the experiment to another.
