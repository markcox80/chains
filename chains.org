#+TITLE: Experiment Chains
#+AUTHOR: Mark Cox

~CHAINS~ is a common lisp system for making it easy to construct an
experiment chain in which a number of links are assembled to produce a
result. Its main purpose is to accommodate scenarios where the line
between a scientific experiment and algorithmic experimentation is
blurred.

[TABLE-OF-CONTENTS]

* A backstory
This section outlines the main use case for the ~CHAINS~ system. The
abstraction the ~CHAINS~ system employs is rather strange when
introduced on its own. Hopefully this section aids in understanding
why. The example presented in this section is real, and is the primary
reason for the existence of the ~CHAINS~ system.

An important problem in the field of computer vision is that of
/congealing/. The task of congealing is very simple, discover the
location of an object contained in an ensemble of images. A key trait
of congealing algorithms is that they know nothing about the object in
each image, other than it can be assumed that the object is present in
each image. A concrete example of congealing would be, given an
ensemble of images containing faces, find me the bounding box of each
face.

When evaluating a congealing algorithm one must evaluate the algorithm
against the following criteria
- Level of initial misalignment :: How far away from the initial guess
     can the algorithm recover the true location of the object?
- Lighting variation :: How does the algorithm handle variation in
     appearance due to illumination conditions?
- Image noise :: How does the algorithm handle variation in appearance
                 due to image noise?
- Appearance variation :: How does the algorithm handle variation in
     appearance of the object? For faces, this would mean male versus
     female, beard versus non-beard, and so on.
- Occlusion :: How does the algorithm handle cases where the object is
               occluded by another object? e.g. a face occluded by
               hair and/or glasses.
- Ensemble size :: How does the number of images in the ensemble
                   influence performance?
- Cross validation :: Testing performance on another ensemble of
     images with exactly the same statistics.

As you have probably discerned by know, the variables mentioned above
are all associated with statistics on the ensemble of images fed to
the congealing algorithm. The reader can assume that these statistics
are in fact controllable. i.e. it is possible to synthesise an
ensemble of images that satisfy the needed statistic.

In addition to variables associated with the input data, a congealing
algorithm typically has a number of parameters that influence its own
performance. These parameters have a range of values that are "worth"
trying.

A scientist also wants to compare the algorithm against another
benchmark algorithm on /exactly/ the same data. If no benchmark
algorithm exists, then a number of other algorithms are
evaluated. Having exactly the same data is extremely important. A
strategically chosen change in data can have a dramatic effect on
results for some algorithms.

Congealing algorithms are typically iterative algorithms. i.e. The
algorithm repeatedly processes its own output until some termination
criteria is satisfied.

Once all of the algorithms are executed, a results collation process
begins. What results that are obtained depend on the paper. These
results may involve performance conditioned on the statistics of the
image ensemble, or results indicating the influence each algorithm
parameter has on its performance.

Everything up to this point is straight forward. You prepare the data
to be fed to the algorithms, you execute the algorithms and then you
produce results (and then write the report/paper/journal). Easy.

Unfortunately, this rarely occurs in practice. 

In practice, one implements an algorithm that has been published. Upon
experimenting (or implementing) with the algorithm, it is discovered
that there is something that could be improved. For example, the
algorithm cannot handle image ensembles with certain statistics or the
algorithm is really sensitive to a particular parameter or the
algorithm is theoretically flawed in some manner. With this new
information, the goal is now to design a new algorithm, that can
overcome the identified algorithm.

Immediately we see that the algorithm and the needed experiments are
not concrete. One typically creates many algorithms that may "do the
job", but one does not know whether it is an idea worth investigating
further until it has been implemented and a preliminary set of results
are obtained. Additionally, the type of results required may change
depending on the point being made.

What a scientist wants to work on is the algorithm. That is what is
important. That is what is publishable. The experiment stuff validates
the argument for the algorithm, but it is not the primary focus. 

Saying it is not of primary focus is rather ironic. The experiment
design is what lead to the discovery. The experiment design also
validates a new discovery. Furthermore, it is the experiment design
where much of a computer scientist's effort is spent and it is to easy
to see why.

The problem with experiments is that they take time to
execute. Obviously this time is a function of the algorithm execution
time. It is also a function of the number of experiments, or what is
termed, the size of the experiment design. An experiment design
defines the experiments to be executed i.e. the different types of
input data, the different types of algorithms, and also the different
types of information required to compute results.

Tragically, the size of the experiment design is a combinatorial
function built from the number of values specified for each experiment
parameter. i.e. If you have a large number of experiment parameters
and a large number of values per experiment parameter then you have an
extremely large number of experiments.

Fortunately, since the experiments are independent, they can be
executed in parallel. Well sort of. 

- Combinatorial explosion :: The number of experiments to execute
     increases combinatorially with the number of experiment parameters. Perform
- Data parallelisation :: Running things sequentially is prohibitive
     with respect to time. It is an embarrassingly parallel problem so
     implement it accordingly.
- Serialisation :: The output of all stages of the experiment need to
                   be saved in order to compute results quickly.
- Feature creep :: When a new part of the experiment design is
                   introduced it must also fit in nicely with the
                   existing implementation.

What the ~CHAINS~ system tries to do, is make it easier to perform
experimentation in order to get results.
