#+TITLE: Experiment Chains
#+AUTHOR: Mark Cox

~CHAINS~ is a common lisp system for making it easy to construct an
experiment chain in which a number of links are assembled to produce a
result. Its main purpose is to accommodate scenarios where the line
between a scientific experiment and algorithmic experimentation is
blurred.

[TABLE-OF-CONTENTS]

* A backstory
This section outlines the main use case for the ~CHAINS~ system. The
abstraction the ~CHAINS~ system employs is rather strange when
introduced on its own. Hopefully this section aids in understanding
why. The example presented in this section is real, and is the primary
reason for the existence of the ~CHAINS~ system.

An important problem in the field of computer vision is that of
/congealing/. The task of congealing is very simple, discover the
location of an object contained in an ensemble of images. A key trait
of congealing algorithms is that they know nothing about the object in
each image, other than it can be assumed that the object is present in
each image. A concrete example of congealing would be, given an
ensemble of images containing faces, find me the bounding box of each
face.

When evaluating a congealing algorithm one must evaluate the algorithm
against the following criteria
- Level of initial misalignment :: How far away from the true location
     can then algorithm correctly locate the object?
- Lighting variation :: How does the algorithm handle variation in
     appearance due to illumination conditions?
- Image noise :: How does the algorithm handle variation in appearance
                 due to image noise?
- Appearance variation :: How does the algorithm handle variation in
     appearance of the object? For faces, this would mean male versus
     female, beard versus non-beard, and so on.
- Occlusion :: How does the algorithm handle cases where the object is
               occluded by another object? e.g. a face occluded by
               hair, glasses or another object.
- Ensemble size :: How does the number of images in the ensemble
                   influence performance?
- Cross validation on images :: Testing performance on another ensemble of
     images with exactly the same statistics.
- Cross validation on geometry :: Testing performance on the same
     collection of images but with different random starting
     locations.

It is fine if you do not understand all of the above. The key point is
that they are /variables/ or statistics measured on the ensemble of
images fed to the congealing algorithm. Congealing algorithms behave
differently depending on the value of each variables. One should
understand that is possible for two different image ensembles to have
the same statistics.

In addition to variables associated with the input data, a congealing
algorithm typically has a number of parameters that influence its own
performance. These parameters have a range of values that are "worth"
trying, with the best value selected at the end.

One also must compare the algorithm against another algorithm on
/exactly/ the same image ensemble. Having exactly the ensemble is
extremely important as a strategically chosen change can have a
dramatic effect on results for some algorithms.

Once all of the algorithms are executed, a results collation process
begins. The results obtained depend on the report or paper that is
being written. These results may involve performance conditioned on
the statistics of the image ensemble, or results indicating the level
of influence that an algorithm parameter has on the overall algorithm
performance. 

Everything up to this point is straight forward. You prepare the image
ensemble or data which is fed to the congealing algorithms, you
execute each algorithm, prepare the results and then write the
report/paper. Easy.

Unfortunately, this rarely occurs in practice. In practice, one
implements an algorithm that has been published. Upon experimenting
(or implementing) with the algorithm, it is discovered that there is
something that could be improved. For example, the algorithm cannot
handle image ensembles with certain statistics or the algorithm is
really sensitive to a particular parameter or the algorithm is
theoretically flawed in some manner. With this new information, the
goal is now to design a new algorithm, that can overcome the
identified problem.

Immediately we see that the algorithm and the experiments to be
performed are not concrete. One typically creates many algorithms that
may "do the job", but one does not know whether it is an idea worth
investigating further until it has been implemented and a preliminary
set of results are obtained. Additionally, the type of results
required may change depending on the point being made.

What a scientist wants to work on is the algorithm. That is what is
important. That is what is publishable. The experiment stuff validates
the argument for the algorithm, but it is not the primary focus. 

Saying it is not of primary focus is rather ironic. The experiment
design is what lead to the discovery. The experiment design also
validates a new discovery. Furthermore, it is the experiment design
where much of a computer scientist's effort is spent and it is to easy
to see why.

The problem with experiments is that they take time to execute and to
implement. The long execution time is due to the number of the
experiments being a combinatorial function built from the number of
values specified for each experiment parameter. i.e. if you have a
large number of experiment parameters and a large number of values per
experiment parameter then you have an /extremely/ large number of
experiments.

Fortunately, since the experiments are independent, they can be
executed in parallel. Well not quite in parallel as describe later,
but the experiment execution problem does belong to the class of
parallelisable problems which I would consider embarrassingly simple
to achieve.

Another difficulty is collating results. When writing a report or a
paper, one typically needs to additional graphs (or tables) that were
not part of the original set of experiments. To accommodate this use
case, the output of the experiments should be saved. This
serialisation not only applies to the algorithm output and input, but
for all of the experiment parameters as well.

This database of serialised experiment data also needs to have the
ability to be queried as well. e.g. obtain all of the experiments that
have this value for this experiment parameter. This is important for
generating results and inspecting input and output.

All of what this is simple to state, but is moderately difficult to
implement and very easy to make simple mistakes which are hard to
identify. Additionally, the code that performs the experiment setup,
execution and result collation very rarely receive the level of
attention and care that algorithms receive. (Who releases code for
their experiments?).

The ~CHAINS~ system attempts to provide facilities for the above tasks
in order for the scientist to spend more time on developing
algorithms. 

The ~CHAINS~ is not a plug and play pipe connecting work flow
optimiser. It should not be a dependency in your algorithm
implementations. Its domain is very well constrained to the problem
defined above, specifically, the only place where the ~CHAINS~ system
should be used is in verifying and validating an algorithm's
performance on different types of data and collating algorithm output
for publication.

* Not sure where this goes yet.
Consider the following where
two congealing algorithms need to process differing input data.
#+begin_src ditaa :file parallelism.png
+--------------+     +-------------+
| Input Data 1 +-+-->| Algorithm 1 |
+--------------+ |   +-------------+
                 |
                 |   +-------------+
                 +-->| Algorithm 2 |
                     +-------------+

+--------------+     +-------------+
| Input Data 2 +-+-->| Algorithm 1 |
+--------------+ |   +-------------+
                 |
                 |   +-------------+
                 +-->| Algorithm 2 |
                     +-------------+
#+end_src
Once input data 1 and 2 are created, all four algorithm invocations
can be executed in parallel provided the algorithm implementation
permits.

For congealing, creating the input data is arguably the most
complicated component of the experimental setup as there are a number
of parameters that govern how the input data is formed. A
visualisation of this process is as follows
#+begin_src ditaa :file input-data.png
  +-------------------------------+
  | Image Appearance and Lighting |
  +------+------------------------+
         | 
         v
  +------------------------+
  | Image Cross Validation |<--=-Images selected here
  +------+-----------------+
         |
         v
  +---------------------+
  | Synthetic Occlusion |<--=-- Occluded regions added.
  +------+--------------+
         | 
         o<--=--Input data images created
         |
         v         
  +---------------------------------+
  | Distance away from ground truth |
  +------+--------------------------+
         |
         v
  +----------------------------+
  | Geometric Cross Validation |<--=-Create initial guess
  +----------------------------+
#+end_src
The term cross validation used above refers to the process of
synthesising a set of random samples with the same statistics. Thus
for the image cross validation, an entirely different set of images
with same level of appearance variation and lighting conditions are
selected. Similarly, the geometric cross validation is a new set
initial guesses on where the object is for the same set of
images. There should be a cross validation block for the synthetic
occlusion component too, but we have chosen to omit it for clarity.

For an experiment design, there will be many different image
appearance and lighting statistics used as well as many different
synthetic occlusions so the above /chain/ that creates the input data
is really a single path in a tree that defines the experiment design.

To complicate matters, there may be many other ways of selecting the
input data images. i.e. with no occlusion, with no cross validation,
with image noise and so on. Thus the marker "Input data images
created" really represents the input to the next stage of the
experiment design. In this case, the initial guess of where the object
is in the image. Obviously, in order to form an initial guess, you
need to know the ground truth location of the object in the
image. Assuming this information is available, then synthesising an
initial guess is trivial.

Once the initial guess is created, then the algorithm can be
executed. The algorithm also needs the input data images, thus, the
input to the algorithm is really the input data images and the initial
guess as depicted in the next figure.
#+begin_src ditaa :file algorithm-input.png
         |
         o<--=--Input data images created
         |
+--------+
|        |
|        v                                                  
| +---------------------------------+
| | Distance away from ground truth |
| +------+--------------------------+
|        | 
|        v
| +----------------------------+
| | Geometric Cross Validation |
| +------+---------------------+
|        |
+---+    o<--=--Initial Guess
    |    |
    v    v
  +-----------+
  | Algorithm |
  +-----------+
#+end_src

What is obvious is that there are various stages along the chain that
define the transition from one component of the experiment to another.
