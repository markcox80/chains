#+TITLE: Experiment Chains
#+AUTHOR: Mark Cox

~CHAINS~ is a common lisp system for making it easy to construct an
experiment chain in which a number of links are assembled to produce a
result. Its main purpose is to accommodate scenarios where the line
between a scientific experiment and algorithmic experimentation is
blurred.

[TABLE-OF-CONTENTS]

* A backstory
This section outlines the main use case for the ~CHAINS~ system. The
abstraction the ~CHAINS~ system employs is rather strange when
introduced on its own. Hopefully this section aids in understanding
why. The example presented in this section is real, and is the primary
reason for the existence of the ~CHAINS~ system.

An important problem in the field of computer vision is that of
/congealing/. The task of congealing is very simple, discover the
location of an object contained in an ensemble of images. A key trait
of congealing algorithms is that they know nothing about the object in
each image, other than it can be assumed that the object is present in
each image. A concrete example of congealing would be, given an
ensemble of images containing faces, find me the bounding box of each
face.

When evaluating a congealing algorithm one must evaluate the algorithm
against the following criteria
- Level of initial misalignment :: How far away from the true location
     can then algorithm correctly locate the object?
- Lighting variation :: How does the algorithm handle variation in
     appearance due to illumination conditions?
- Image noise :: How does the algorithm handle variation in appearance
                 due to image noise?
- Appearance variation :: How does the algorithm handle variation in
     appearance of the object? For faces, this would mean male versus
     female, beard versus non-beard, and so on.
- Occlusion :: How does the algorithm handle cases where the object is
               occluded by another object? e.g. a face occluded by
               hair, glasses or another object.
- Ensemble size :: How does the number of images in the ensemble
                   influence performance?
- Cross validation on images :: Testing performance on another ensemble of
     images with exactly the same statistics.
- Cross validation on geometry :: Testing performance on the same
     collection of images but with different random starting
     locations.

It is fine if you do not understand all of the above. The key point is
that they are /variables/ or statistics measured on the ensemble of
images fed to the congealing algorithm. Congealing algorithms behave
differently depending on the value of each variables. One should
understand that is possible for two different image ensembles to have
the same statistics.

In addition to variables associated with the input data, a congealing
algorithm typically has a number of parameters that influence its own
performance. These parameters have a range of values that are "worth"
trying, with the best value selected at the end.

One also must compare the algorithm against another algorithm on
/exactly/ the same image ensemble. Having exactly the ensemble is
extremely important as a strategically chosen change can have a
dramatic effect on results for some algorithms.

Once all of the algorithms are executed, a results collation process
begins. The results obtained depend on the report or paper that is
being written. These results may involve performance conditioned on
the statistics of the image ensemble, or results indicating the level
of influence that an algorithm parameter has on the overall algorithm
performance. 

Everything up to this point is straight forward. You prepare the image
ensemble or data which is fed to the congealing algorithms, you
execute each algorithm, prepare the results and then write the
report/paper. Easy.

Unfortunately, this rarely occurs in practice. In practice, one
implements an algorithm that has been published. Upon experimenting
(or implementing) with the algorithm, it is discovered that there is
something that could be improved. For example, the algorithm cannot
handle image ensembles with certain statistics or the algorithm is
really sensitive to a particular parameter or the algorithm is
theoretically flawed in some manner. With this new information, the
goal is now to design a new algorithm, that can overcome the
identified problem.

Immediately we see that the algorithm and the experiments to be
performed are not concrete. One typically creates many algorithms that
may "do the job", but one does not know whether it is an idea worth
investigating further until it has been implemented and a preliminary
set of results are obtained. Additionally, the type of results
required may change depending on the point being made.

What a scientist wants to work on is the algorithm. That is what is
important. That is what is publishable. The experiment stuff validates
the argument for the algorithm, but it is not the primary focus. 

Saying it is not of primary focus is rather ironic. The experiment
design is what lead to the discovery. The experiment design also
validates a new discovery. Furthermore, it is the experiment design
where much of a computer scientist's effort is spent and it is to easy
to see why.

The problem with experiments is that they take time to execute and to
implement. The long execution time is because the number of
experiments is a combinatorial function defined by the number of
values specified for each experiment parameter. i.e. if you have a
large number of experiment parameters and a large number of values per
experiment parameter then you have an /extremely/ large number of
experiments.

Fortunately, since the experiments are independent, they can be
executed in parallel. Well not quite in parallel as describe later,
but the experiment execution problem does belong to the class of
parallelisable problems which I would consider embarrassingly simple
to achieve.

Another difficulty is collating results. When writing a report or a
paper, one typically needs to additional graphs (or tables) that were
not thought of initially. To accommodate this use case, the output of
the experiments should be saved in order to save (valuable) time. This
serialisation not only applies to the algorithm output and input, but
for all of the experiment parameters as well.

This database of serialised experiment data also needs to have the
ability to be queried as well. This ability is needed by the results
collation step mentioned previously.

All of this is simple to state, but is moderately difficult to
implement and very easy to make simple mistakes which are hard to
identify. Additionally, the code that performs the experiment setup,
execution and result collation very rarely receive the level of
attention and care that algorithms receive. In my local minima, I
haven't seen any one release code just for performing experiments. As
to why, I am not sure.

The ~CHAINS~ system attempts to provide facilities for the above tasks
in order for the scientist to spend more time on developing an
understanding of existing algorithms and to develop ones own
algorithms.

The ~CHAINS~ is not a plug and play work flow optimiser where one
performs plumping. It should not be a dependency in your algorithm
implementations. Its domain is very well constrained to the problem
defined above, specifically, the only place where the ~CHAINS~ system
should be used is in verifying and validating an algorithm's
performance on different types of data and collating algorithm output
for a publication.

* Concepts and mental models
The background section painted a picture of the aspects of the problem
the ~CHAINS~ system attempts to address. This section attempts to
illustrate more of the core concepts that underpin the library.

Performing an experiment is really the serial execution of a number of
blocks. For the congealing problem, the high-level blocks are as
follows
#+begin_src ditaa :file congealing-blocks.png
+------------+
| Input Data |
+-----+------+
      |
      v
+-----------+
| Algorithm |
+-----+-----+
      |
      v
  +--------+
  | Metric |
  +--------+
#+end_src
The input data block is responsible for creating the ensemble of
images fed to the algorithm in order to perform its task. Once
finished, the output of the algorithm is compared to the ground truth
in order to quantitatively evaluate the algorithm on the input data.

This linking of blocks to be executed is termed a chain.

A collection of experiments to be executed is really a tree. Consider
the following figure where there are two congealing algorithms which
are evaluated on differing input data.
#+begin_src ditaa :file parallelism.png
+--------------+     +-------------+   +--------+
| Input Data 1 +-+-->| Algorithm 1 |-->| Metric |
+--------------+ |   +-------------+   +--------+
                 |
                 |   +-------------+   +--------+
                 +-->| Algorithm 2 |-->| Metric |
                     +-------------+   +--------+

+--------------+     +-------------+   +--------+
| Input Data 2 +-+-->| Algorithm 1 |-->| Metric |
+--------------+ |   +-------------+   +--------+
                 |
                 |   +-------------+   +--------+
                 +-->| Algorithm 2 |-->| Metric |
                     +-------------+   +--------+
#+end_src
The above figure presents two trees which start from an input data
block. In the ~CHAINS~ system a hidden root node is used to produce a
single tree. As can be seen, the path from the top of the tree to a
leaf represents a single experiment or execution chain.

The tree view of the experiments also highlights the parallelism of
the experiment execution. i.e. once input data 1 and 2 are created,
all four algorithm invocations can be executed in parallel (provided
that the implementation permits it). Likewise, all four metrics can be
computed once the algorithms are completed.

The ~CHAINS~ system does not limit the length of the chain, nor are
there any constraints placed on the type of experiment tree. In fact
the ~CHAINS~ system does not provide any default blocks. Blocks are
arbitrary problem specific entities. What the ~CHAINS~ system does
provide is a mechanism for constructing a new tree, discovering an
existing tree, executing the tree and querying a tree.

Defining the blocks is one thing, but another important component is
the way the interact. The ~CHAINS~ system assumes that a block can
only depend on output from previous blocks in the chain. This is shown
in the next example for the congealing problem.

For congealing, creating the input data is arguably the most
complicated component of the experimental setup as there are a number
of parameters that govern how the input data is formed. A
visualisation of this process is as follows
#+begin_src ditaa :file input-data.png
  +-------------------------------+
  | Image Appearance and Lighting |
  +------+------------------------+
         | 
         v
  +------------------------+
  | Image Cross Validation |<--=-Images selected here
  +------+-----------------+
         |
         v
  +---------------------+
  | Synthetic Occlusion |<--=-- Occluded regions added.
  +------+--------------+
         | 
         o<--=--Input data images created
         |
         v         
  +---------------------------------+
  | Distance away from ground truth |
  +------+--------------------------+
         |
         v
  +----------------------------+
  | Geometric Cross Validation |<--=-Create initial guess
  +----------------------------+
#+end_src
The term cross validation used above refers to the process of
synthesising a set of random samples with the same statistics. Thus
for the image cross validation, an entirely different set of images
with same level of appearance variation and lighting conditions are
selected. Similarly, the geometric cross validation is a new set
initial guesses on where the object is for the same set of
images. There should be a cross validation block for the synthetic
occlusion component too, but we have chosen to omit it for clarity.

To complicate matters, there may be many other ways of selecting the
input data images. i.e. with no occlusion, with no cross validation,
with image noise and so on. Thus the marker "Input data images
created" really represents the input to the next stage of the
experiment. In this case, computing the initial guess of where the
object is in the image. Obviously, in order to form an initial guess,
you need to know the ground truth location of the object in the
image. Assuming this information is available, then synthesising an
initial guess is trivial.

Once the initial guess is created, then the algorithm can be
executed. The algorithm also needs the input data images, thus, the
input to the algorithm is really the input data images and the initial
guess as depicted in the next figure.
#+begin_src ditaa :file algorithm-input.png
         |
         o<--=--Input data images created
         |
+--------+
|        |
|        v                                                  
| +---------------------------------+
| | Distance away from ground truth |
| +------+--------------------------+
|        | 
+---+    |
|   |    |
|   v    v
| +----------------------------+
| | Geometric Cross Validation |
| +------+---------------------+
|        |
+---+    o<--=--Initial Guess
    |    |
    v    v
  +-----------+
  | Algorithm |
  +-----------+
#+end_src

The above diagram is no longer a chain, nor is it a tree, it is a
graph. How the ~CHAINS~ system models the above graph is by
distinguishing between blocks that perform an operation and the
output of that operation. 

An example of the ~CHAINS~ system in use for the above graph is as
follows. Lets consider the "Geometric Cross Validation" block. The
following code defines the appropriate block.
#+begin_src lisp
  (define-block geometric-cross-validation ()
    ((sample
      :initarg sample
      :reader sample
      :documentation "The index of the random sample.")))
#+end_src

The following code defines the action that is to performed when the
above block is executed.
#+begin_src lisp
  (define-input input-data)
  (define-input distance-away-from-ground-truth)
  
  (define-operation (object geometric-cross-validation)
      ((input-data input-data)
       (distance distance-away-from-ground-truth))
    (synthesize-initial-guess input-data distance))
#+end_src

The arguments ~INPUT-DATA~ and ~DISTANCE-AWAY-FROM-GROUND-TRUTH~ are
special arguments that represent the input to the
~GEOMETRIC-CROSS-VALIDATION~ block. 

The value of the places ~INPUT-DATA~ and ~DISTANCE~ vary depending on
the chain being executed. In order to compute the input values for a
given block and chain you use the ~DEFINE-LINK~ macro.
#+begin_src lisp
  (define-link input-data geometric-cross-validation ((occlusion synthetic-occlusion))
    (block-value occlusion))
  
  (define-link distance-away-from-ground-truth geometric-cross-validation ((object distance-away-from-ground-truth))
    (distance object))
#+end_src

The function ~BLOCK-VALUE~ in the link definition for ~INPUT-DATA~ is
a special function that returns the output of the operation for the
~OCCLUSION~ block.

The ~DEFINE-INPUT~ and ~DEFINE-LINK~ provide a flexible method of
computing input values to a block conditioned on the blocks in the
chain. The ~DEFINE-OPERATION~ macro also permits altering the
operation performed based on the blocks in the chain.

#+begin_src lisp
  (define-operation (object appearance-cross-validation)
      ((variation appearance-and-lighting))
    ;; Synthesize a set of images with appearance and lighting variation.
    )
  
  (define-operation (object appearance-cross-validation)
      ((variation appearance))
    ;; Synthesize a set of images with appearance variation.
    )
#+end_src

Further details on the ~DEFINE-OPERATION~ and ~DEFINE-LINK~ are found
later in this document.

* Serialisation
The serialisation strategy employed in the ~CHAINS~ system is a very
simple system involving only the lisp pretty printer
(~*PRINT-READABLY*~ is ~T~) as the data format, and only files and
directories on the storage device. 

Each node of an experiment tree has its own directory in order to
store the block parameters, the block value generated by
~BLOCK-OPERATION~ and other block specific data. The name of this
directory is computed using the function ~BLOCK-NAME~.
#+begin_src lisp
(defgeneric block-name (block))
#+end_src

The macro ~EASY-BLOCK-NAME~ can be used to automatically generate the
implementation of ~BLOCK-NAME~ for a given block.

Serialisation of blocks is performed using the function
~SERIALISE-BLOCK~. This function produces a string that when read and
evaluated, produces a new instance of the block which is considered
/equal/ to the block that was serialised. This process is similar to
the ~*PRINT-READABLY*~ functionality, but does not rely on
~PRINT-OBJECT~. 

~SERIALISE-BLOCK~ iterates through all slots of the block,
transforming each slot value in to an expression which evaluates to an
/equal/ object. Customisation of the generated expression for a given
object is provided by the ~READABLE-VALUE~ generic function or by the
slot definition option ~:READABLE-VALUE-FUNCTION~ for block
definitions.

* Querying
Querying a set of experiments is critical to the collation of
results. When a set of experiments have finished executing, the
experiment chains can be discovered using the function
~DISCOVER-CHAINS~.
#+begin_src lisp
  (defgeneric discover-chains (pathname))
#+end_src
This function returns a list of ~CHAIN~ instances. A chain instance
contains the blocks that formed the experiment.

The function ~CONTAINS-BLOCK-P~ can be used to determine if a chain
contains the given block.
#+begin_src lisp
  (defun contains-block-p (chain block))
#+end_src

The function ~FIND-CHAINS-WITH-BLOCK~ returns all chains in which
~CONTAINS-BLOCK-P~ is ~T~ for the given block.
#+begin_src lisp
  (defun find-chains-with-block (chains block))
#+end_src

Another important function is ~GROUP-BY~, which groups chains
together according to given a test.
#+begin_src lisp
  (defun group-by (sequence test))
#+end_src

In fact, the ~GROUP-BY~ function is needed so frequently, that a
special function ~GROUP-CHAINS~ is provided.

* Generating
